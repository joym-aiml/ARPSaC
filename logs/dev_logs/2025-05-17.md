# 📅 Dev Log — 2025-05-17

**Project:** ARPSaC — Agentic AI for Automated Research Paper Summarization and Critique  
**Focus:** Infrastructure Setup, Connectivity Checks, and Firestore Initialization

---

## ✅ Summary

Today’s work focused on establishing the foundational infrastructure for ARPSaC. We completed environment setup, validated external services, configured secure connections, and verified core tooling — laying the groundwork for agent development and experimentation.

---

## 📂 Tasks Completed

### 🔧 Environment & Repository Setup
- [x] Initialized Python virtual environment (`venv`)
- [x] Created `.gitignore` tailored to preserve experiments, logs, and code decisions
- [x] Set up full project directory scaffold (modular research-first structure)
- [x] Installed all core libraries via `requirements.txt`

### 🛠️ Firestore (Google Cloud)
- [x] Created Firebase project in **Native Mode**
- [x] Generated and downloaded **service account credentials**
- [x] Connected Firestore via `firebase-admin` SDK
- [x] Wrote and verified first document in Firestore
- [x] Modularized `firestore_interface.py`

### 🧠 FAISS Vector Store
- [x] Installed `faiss-cpu`
- [x] Created a FAISS flat index with dummy 384D vectors
- [x] Performed similarity search and verified output
- [x] Explained vector indexing line-by-line for transparency

### 📦 Toolchain Validation
- [x] Verified Python venv is active and isolated
- [x] Confirmed Firestore credentials are excluded via `.gitignore`
- [x] ADK CLI installed and ready (`adk --help`)
- [x] Jupyter Lab and Notebooks installed and launch successfully
- [x] Optional: Streamlit available for future UI (tested via `streamlit hello`)

---

## 🔍 Key Takeaways

- ✅ Firestore connection and document writes are working smoothly.
- ✅ FAISS is indexing and retrieving vectors correctly (ready for embedding use).
- ✅ Project is set up with reproducibility, modularity, and experiment tracking in mind.

---

## 🧭 Next Steps

- [ ] Create reusable Firestore write/read functions
- [ ] Start experimentation notebook for summarizer in `/experiments/summarization/`
- [ ] Set up `fetch_papers.py` as the first agent under `/agents/`
- [ ] Define the ADK pipeline skeleton in `adk_config/pipelines.json`

---

## 🧪 Summarization Experiments

Today, we initiated structured experimentation on baseline summarization models to compare performance on academic abstracts. The goal was to evaluate output quality and task suitability using both human review and automatic metrics.

---

### 🧬 Model Experiments Conducted

| Model                  | Hugging Face ID             | Status     |
|------------------------|-----------------------------|------------|
| `flan-t5-small`        | `google/flan-t5-small`      | ✅ Tested  |
| `bart-large-cnn`       | `facebook/bart-large-cnn`   | ✅ Tested  |
| `pegasus-xsum`         | `google/pegasus-xsum`       | ✅ Tested  |

---

### ⚙️ Tasks Completed

- [x] Successfully loaded and executed `flan-t5-small` with prompt-based summarization
- [x] Used BART through `pipeline()` interface and validated its output style
- [x] Resolved Pegasus loading warning and tested `google/pegasus-xsum` on academic input
- [x] Compared summaries for fluency, coverage, and informativeness
- [x] Logged reasoning behind prompt formats, tokenizer types, and generation controls

---

### 📈 Evaluation Infrastructure

- [x] Installed `evaluate` and `bert-score` for metric-based summary evaluation
- [x] Created `evaluation.ipynb` to calculate ROUGE and BERTScore
- [x] Added placeholders for model outputs and reference summaries
- [x] Outlined scoring and comparative logging process

---

## 📌 Reflection

This session transitioned ARPSaC from infrastructure validation into model experimentation. With summaries from three baselines now compared and evaluation tools in place, we are ready to optimize model selection and feed the best performer into the agentic pipeline.

